# Chatbot System

A comprehensive multi-client chatbot system with enhanced training pipeline, admin dashboard, and vector-based AI processing.

## ğŸ—ï¸ Project Structure (3 Main Categories)

```
chatbot/
â”œâ”€â”€ ğŸ“ chatbot/              # Core chatbot engine, training, and all AI functionality
â”‚   â”œâ”€â”€ app.py              # Main chatbot Flask application
â”‚   â”œâ”€â”€ chatbot_engine.py   # Chatbot response engine
â”‚   â”œâ”€â”€ knowledge_base.py   # Knowledge management system
â”‚   â”œâ”€â”€ scraper.py          # Web scraping functionality
â”‚   â”œâ”€â”€ training_pipeline.py # Complete 5-step training system
â”‚   â”œâ”€â”€ vectorize_data.py   # Legacy vectorization
â”‚   â”œâ”€â”€ integration_example.py # Integration examples
â”‚   â”œâ”€â”€ config.json         # Chatbot configuration
â”‚   â”œâ”€â”€ logging_config.py   # Logging configuration
â”‚   â”œâ”€â”€ start_admin_only.py # Admin startup script
â”‚   â”œâ”€â”€ start_admin_system.py # Full system startup
â”‚   â””â”€â”€ documentation files # Training and system docs
â”œâ”€â”€ ğŸ“ admin_dashboard/      # Admin dashboard and management tools
â”‚   â”œâ”€â”€ admin_dashboard.py  # Main admin dashboard
â”‚   â”œâ”€â”€ client_management.py # Client account management
â”‚   â”œâ”€â”€ enhanced_app.py     # Enhanced client features
â”‚   â”œâ”€â”€ dashboard.html      # Dashboard UI
â”‚   â””â”€â”€ README_CLIENT_SYSTEM.md # Client system documentation
â”œâ”€â”€ ğŸ“ data/                # All data storage and databases
â”‚   â”œâ”€â”€ knowledge/         # Per-client knowledge storage
â”‚   â”œâ”€â”€ sessions.csv       # Chat sessions
â”‚   â”œâ”€â”€ usage_logs.csv     # Usage analytics
â”‚   â””â”€â”€ uploads/           # File uploads
â”œâ”€â”€ ğŸ“ chatbot_env/         # Python virtual environment
â””â”€â”€ README.md              # This file
```

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Activate virtual environment
source chatbot_env/bin/activate  # Linux/Mac
# or
chatbot_env\Scripts\activate     # Windows

# Install base dependencies
pip install -r requirements.txt
```

### 2. Training Pipeline Setup (New!)
```bash
# Install training dependencies
pip install -r chatbot/requirements.txt

# Install and setup Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull nomic-embed-text
ollama serve
```

### 3. Start the System
```bash
# Option 1: Full system (admin + chatbot + training)
python chatbot/start_admin_system.py

# Option 2: Admin dashboard only
python chatbot/start_admin_only.py

# Option 3: Chatbot API only
cd chatbot && python app.py
```

## ğŸ¯ Key Features

### âœ¨ Enhanced Training Pipeline (New!)
- **5-Step AI Training Process**:
  1. ğŸ“¥ Get the info (web scraping, manual input)
  2. ğŸ” Analyze the info (quality assessment, readability)
  3. âœ‚ï¸ Chunk into smaller pieces (intelligent text chunking)
  4. ğŸ¤– AI vectorization using **Ollama** (local AI processing)
  5. ğŸ’¾ Save to CSV (enhanced storage with vectors)

### ğŸ›ï¸ Admin Dashboard
- Multi-client management
- Knowledge base administration
- Usage analytics and monitoring
- Training pipeline integration

### ğŸ’¬ Chatbot Engine
- Context-aware conversations
- Company-specific knowledge base
- Real-time response generation
- Session management

### ğŸ“Š Client Management
- Individual client accounts
- Isolated knowledge bases
- Usage tracking and limits
- API key management

## ğŸ“– Documentation

- **[Training Pipeline Guide](chatbot/TRAINING_PIPELINE_README.md)** - Complete guide to the AI training system
- **[Admin Dashboard Guide](chatbot/ADMIN_DASHBOARD_README.md)** - Admin interface documentation
- **[Integration Examples](chatbot/integration_example.py)** - Code examples and integration patterns

## ğŸ”§ Configuration

### Chatbot Configuration
Edit `chatbot/config.json` for chatbot settings:
- Server settings (host, port)
- Scraper configuration
- Knowledge base limits
- Security settings

### Training Configuration
The training pipeline can be configured in `chatbot/training_pipeline.py`:
```python
pipeline = TrainingPipeline(
    ollama_model="nomic-embed-text",  # Embedding model
    chunk_size=300,                  # Words per chunk
    overlap=30,                      # Overlap between chunks
    data_dir="./data"               # Storage directory
)
```

## ğŸ”„ Workflow

### Training New Content
```python
from chatbot.training_pipeline import TrainingPipeline

pipeline = TrainingPipeline()

# Process text content
result = pipeline.process_content(
    content="Your company information...",
    company_id="company-123",
    category="services"
)

# Or process from URL
result = pipeline.process_from_url(
    url="https://company.com/about",
    company_id="company-123"
)
```

### Using in Chatbot
The processed data automatically integrates with your existing chatbot system through the enhanced CSV storage format.

## ğŸ“¦ Dependencies

### Core System
- Flask 3.1.1
- BeautifulSoup4 4.13.4
- Requests 2.32.4

### Training Pipeline (New)
- LangChain Community 0.3.19
- NLTK 3.8.1
- TextStat 0.7.4
- Ollama (local installation)

### Optional
- Pandas (data analysis)
- Pytest (testing)

## ğŸŒŸ What's New

### Enhanced Training Pipeline
- **Local AI Processing**: Uses Ollama instead of external APIs
- **Intelligent Analysis**: Content quality scoring and optimization
- **Smart Chunking**: Context-aware text segmentation
- **Vector Storage**: Embeddings saved in CSV format
- **Quality Monitoring**: Detailed analytics on training effectiveness

### Improved Organization
- **Modular Structure**: Clear separation of concerns
- **Better Documentation**: Comprehensive guides for each component
- **Example Code**: Integration patterns and usage examples
- **Configuration Management**: Centralized config files

## ğŸ¤ Contributing

1. Follow the 3-category structure (chatbot, client_manager, data)
2. Add tests for new features
3. Update documentation
4. Keep related files together in their category

## ğŸ“ Support

- Check the `chatbot/` directory for detailed guides and documentation
- Review `chatbot/integration_example.py` for integration patterns
- Examine logs for troubleshooting
- Ensure all dependencies are properly installed

---

**Note**: This system now includes advanced AI training capabilities with local processing via Ollama, making it more powerful and cost-effective than before!